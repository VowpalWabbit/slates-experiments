{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-d Contextual Bandit with Slates on Simulated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vowpalwabbit import pyvw\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import math\n",
    "import slates\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_outcomes():\n",
    "    return {('Mac', 'wifi', 'CA'): [], ('Mac', 'wifi', 'US'): [], ('Mac', 'wired', 'CA'): [], ('Mac', 'wired', 'US'): [], ('Windows', 'wifi', 'CA'): [], ('Windows', 'wifi', 'US'): [], ('Windows', 'wired', 'CA'): [], ('Windows', 'wired', 'US'): []}\n",
    "\n",
    "def optimal_policy_sample(context, action, name, sample_size=1):\n",
    "    temp_df = ground_truth_info[name]['ground_truth_rewards_df']\n",
    "    row_index = temp_df.index[(temp_df['platform'] == context[0])\n",
    "                                           & (temp_df['network'] == context[1])\n",
    "                                           & (temp_df['country'] == context[2])\n",
    "                                           & (temp_df['x'] == action[0])\n",
    "                                           & (temp_df['y'] == action[1])\n",
    "                                           & (temp_df['z'] == action[2])\n",
    "                                          ]\n",
    "    possible_rewards = temp_df.iloc[row_index[0]][\"reward\"]\n",
    "    return np.random.choice(possible_rewards, sample_size, replace=True)\n",
    "\n",
    "def optimal_policy_median(context, action, name):\n",
    "    temp_df = ground_truth_info[name]['ground_truth_rewards_df']\n",
    "    row_index = temp_df.index[(ground_truth_rewards_df['platform'] == context[0])\n",
    "                                           & (temp_df['network'] == context[1])\n",
    "                                           & (temp_df['country'] == context[2])\n",
    "                                           & (temp_df['x'] == action[0])\n",
    "                                           & (temp_df['y'] == action[1])\n",
    "                                           & (temp_df['z'] == action[2])\n",
    "                                          ]\n",
    "    possible_rewards = temp_df.iloc[row_index[0]][\"reward\"]\n",
    "    return np.median(possible_rewards)\n",
    "\n",
    "def plot_rewards(log=False):\n",
    "    contexts = sorted(list(test_configs[next(iter(test_configs))][\"outcomes\"].keys()))\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    for context in contexts:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i, name in enumerate(test_configs):\n",
    "            samples = test_configs[name][\"outcomes\"][context]\n",
    "            window_size = int(len(samples) / 10)\n",
    "            plot_data = pd.Series(samples).rolling(window_size, min_periods=0).mean()\n",
    "            if log:\n",
    "                plot_data = np.log(plot_data)\n",
    "            optimal_policy_results = optimal_policy_sample(context, ground_truth_info[name]['min_actions'][context], name, len(samples))\n",
    "            plot_opt = pd.Series(optimal_policy_results).rolling(int(len(samples)/10), min_periods=0).mean()\n",
    "            if log:\n",
    "                plot_opt = np.log(plot_opt)\n",
    "            plt.plot(plot_data, color=colors[i], linewidth=2, label=\"{} slate\".format(name))\n",
    "            plt.plot(plot_opt, color=colors[i], linewidth=2, linestyle=':', label=\"Best Possible\")\n",
    "        ymin = 0.05\n",
    "        if log:\n",
    "            ymin = np.log(0.05)\n",
    "        plt.axhline(y=ymin, color='k', linestyle=':', label='Optimal(Groundtruth)')\n",
    "        plt.xlim([0, 400])\n",
    "        plt.title(\"{}\".format(context))\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify files and contexts of the experiment. These data files are generated by using the Continuous Multi-D Simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'E:\\data\\20200214_vector_plearning_msrnyc\\data'\n",
    "GROUND_TRUTH_DATASETS = {\n",
    "     \"4_3_2_0,1\" : \"df_all_4_3_2_0.01.csv\",\n",
    "    \"8_6_4_0,1\" : \"df_all_8_6_4_0.01.csv\",\n",
    "     \"16_12_8_0,1\" : \"df_all_16_12_8_0.01.csv\",\n",
    "#      \"32_24_16_0,1\" : \"df_all_32_24_16_0.01.csv\"\n",
    "}\n",
    "\n",
    "TEST_DATASETS = {\n",
    "     \"4_3_2_0,1\" : \"df_all_4_3_2_0.01.csv\",\n",
    "    \"8_6_4_0,1\" : \"df_all_8_6_4_0.01.csv\",\n",
    "     \"16_12_8_0,1\" : \"df_all_16_12_8_0.01.csv\",\n",
    "#      \"32_24_16_0,1\" : \"df_all_32_24_16_0.01.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_to_choose_from = [\n",
    "    ('Mac','wifi','CA') ,\n",
    "    ('Mac','wifi','US'),\n",
    "    ('Mac','wired','CA'),\n",
    "    ('Mac','wired','US'),\n",
    "    ('Windows','wifi','CA'),\n",
    "    ('Windows','wifi','US'),\n",
    "    ('Windows','wired','CA'),\n",
    "    ('Windows','wired','US')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning parameters: VW commands and iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = \"--quiet --cb_type mtr --epsilon 0.2 --first_only\"\n",
    "slates_args = common_args + \" --ccb_explore_adf --coin --interactions UUUA\" \n",
    "cb_args = common_args + \" --cb_explore_adf --coin --interactions UUUA\"\n",
    "num_iter = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all datasets to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_configs = {}\n",
    "for name in TEST_DATASETS:\n",
    "    test_configs[name] = {}\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, TEST_DATASETS[name]))\n",
    "    test_configs[name][\"data\"] = df\n",
    "    test_configs[name][\"rewards\"] = pd.DataFrame(df.groupby(['platform', 'network', 'country','x','y','z'])['reward'].unique()).reset_index()\n",
    "    test_configs[name][\"x\"] = sorted(df[\"x\"].unique())\n",
    "    test_configs[name][\"y\"] = sorted(df[\"y\"].unique())\n",
    "    test_configs[name][\"z\"] = sorted(df[\"z\"].unique())\n",
    "    test_configs[name][\"x_actions\"] = [\"x=\"+str(a) for a in test_configs[name][\"x\"]]\n",
    "    test_configs[name][\"y_actions\"] = [\"y=\"+str(a) for a in test_configs[name][\"y\"]]\n",
    "    test_configs[name][\"z_actions\"] = [\"z=\"+str(a) for a in test_configs[name][\"z\"]]\n",
    "    all_string_actions, all_actions = slates.combine_float_actions_categorical(test_configs[name][\"x\"],test_configs[name][\"y\"],test_configs[name][\"z\"])\n",
    "    test_configs[name][\"all_string_actions\"] = all_string_actions\n",
    "    test_configs[name][\"all_actions\"] = all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal action/reward from the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_info = {}\n",
    "\n",
    "for name in GROUND_TRUTH_DATASETS:\n",
    "    ground_truth_info[name] = {}\n",
    "    ground_truth_df = pd.read_csv(os.path.join(DATA_PATH, GROUND_TRUTH_DATASETS[name]))\n",
    "    ground_truth_rewards_df = pd.DataFrame(ground_truth_df.groupby(['platform', 'network', 'country','x','y','z'])['reward'].unique()).reset_index()\n",
    "\n",
    "    min_reward = {}\n",
    "    min_actions = {}\n",
    "\n",
    "    grps_context = ground_truth_df.groupby(['platform', 'network', 'country'])\n",
    "    for i, context in enumerate(grps_context.groups.keys()):\n",
    "        df_temp = grps_context.get_group(context)\n",
    "        if context not in min_reward.keys():\n",
    "            min_reward[context] = {}\n",
    "        grps_action = df_temp.groupby(['x', 'y', 'z'])\n",
    "        for action in grps_action.groups.keys():\n",
    "            df_temp2 = grps_action.get_group(action)\n",
    "            min_reward[context][action] = np.mean(df_temp2['reward'])\n",
    "\n",
    "        min_reward_action = min(min_reward[context], key=min_reward[context].get)\n",
    "        min_actions[context] = min_reward_action\n",
    "        \n",
    "    ground_truth_info[name]['min_reward'] = min_reward\n",
    "    ground_truth_info[name]['min_actions'] = min_actions\n",
    "    ground_truth_info[name]['ground_truth_rewards_df'] = ground_truth_rewards_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trajectory_strings = []\n",
    "for name in test_configs:\n",
    "    \n",
    "    print('Running slates on {0}'.format(name))\n",
    "    test_configs[name][\"outcomes\"] = setup_outcomes()\n",
    "    test_configs[name][\"x_outcomes\"] = setup_outcomes()\n",
    "    test_configs[name][\"y_outcomes\"] = setup_outcomes()\n",
    "    test_configs[name][\"z_outcomes\"] = setup_outcomes()\n",
    "    \n",
    "    model = pyvw.vw(slates_args)\n",
    "\n",
    "    df = test_configs[name][\"data\"]\n",
    "    rewards = test_configs[name][\"rewards\"]\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        \n",
    "        # Get data\n",
    "        platform,network,country = contexts_to_choose_from[np.random.choice(len(contexts_to_choose_from))]\n",
    "        shared_context = \"platform={} region={} connection={}\".format(platform, country, network)\n",
    "        examples = slates.create_slates_example(model, shared_context, [test_configs[name][\"x_actions\"], test_configs[name][\"y_actions\"], test_configs[name][\"z_actions\"]])\n",
    "        \n",
    "        # Pred\n",
    "        pred = slates.slate_pred_conv(model.predict(examples, prediction_type=pyvw.pylibvw.vw.pDECISION_SCORES))\n",
    "        model.finish_example(examples)\n",
    "                \n",
    "        # Choose the slot to samlpe\n",
    "        chosen_slot = np.random.choice(len(pred))\n",
    "        slot_to_sample = pred[chosen_slot]\n",
    "        \n",
    "        # Sample an index from this slot\n",
    "        index = slates.sample_index(slot_to_sample)\n",
    "        \n",
    "        # Swap sampled action if it was not the 0th item.\n",
    "        if index != 0:\n",
    "            slot_to_sample[0], slot_to_sample[index] = slot_to_sample[index], slot_to_sample[0]\n",
    "            \n",
    "        # Assign the potentially modified slot back into the prediction\n",
    "        pred[chosen_slot] = slot_to_sample\n",
    "        \n",
    "        exploit_a = 0\n",
    "        for pred_a in pred:\n",
    "            all_probs = [x[1] for x in pred_a]\n",
    "            if pred_a[0][0] == max(pred_a,key=lambda x:x[1])[0] and not(all_probs[1:] == all_probs[:-1]):\n",
    "                exploit_a +=1\n",
    "\n",
    "        chosen_x = test_configs[name][\"x\"][pred[0][0][0]]\n",
    "        chosen_y = test_configs[name][\"y\"][pred[1][0][0]]\n",
    "        chosen_z = test_configs[name][\"z\"][pred[2][0][0]]\n",
    "        \n",
    "        trajectory_strings.append(f\"\\\"('{platform}', '{network}', '{country}')\\\",\\\"({chosen_x},{chosen_y},{chosen_z})\\\",1\")\n",
    "               \n",
    "        row_index = rewards.index[(rewards['platform'] == platform)\n",
    "                                       & (rewards['network'] == network)\n",
    "                                       & (rewards['country'] == country)\n",
    "                                       & (rewards['x'] == chosen_x)\n",
    "                                       & (rewards['y'] == chosen_y)\n",
    "                                       & (rewards['z'] == chosen_z)\n",
    "                                      ]\n",
    "        # Choose a reward from the set that matched this example\n",
    "        possible_rewards = rewards.iloc[row_index[0]][\"reward\"]\n",
    "        cost = np.random.choice(possible_rewards)\n",
    "\n",
    "        x_index = test_configs[name][\"x_actions\"].index(\"x=\"+str(chosen_x))\n",
    "        y_index = test_configs[name][\"y_actions\"].index(\"y=\"+str(chosen_y))\n",
    "        z_index = test_configs[name][\"z_actions\"].index(\"z=\"+str(chosen_z))\n",
    "        x_outcome = (x_index, cost, pred[0][0][1])\n",
    "        y_outcome = (y_index, cost, pred[1][0][1])\n",
    "        z_outcome = (z_index, cost, pred[2][0][1])\n",
    "        \n",
    "        # Only save the outcome for plotting if it was exploit\n",
    "        if exploit_a == 3:\n",
    "            test_configs[name][\"outcomes\"][(platform,network,country)].append(cost)\n",
    "            test_configs[name][\"x_outcomes\"][(platform,network,country)].append(test_configs[name][\"x\"][x_index])\n",
    "            test_configs[name][\"y_outcomes\"][(platform,network,country)].append(test_configs[name][\"y\"][y_index])\n",
    "            test_configs[name][\"z_outcomes\"][(platform,network,country)].append(test_configs[name][\"z\"][z_index])\n",
    "\n",
    "        examples = slates.create_slates_example(model, shared_context, [test_configs[name][\"x_actions\"], test_configs[name][\"y_actions\"], test_configs[name][\"z_actions\"]], [x_outcome,y_outcome,z_outcome])\n",
    "        model.learn(examples)\n",
    "        model.finish_example(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'slate_trajectory2.csv'), 'w') as f:\n",
    "    for line in trajectory_strings:\n",
    "        f.write(line + os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear scale plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log scale plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
