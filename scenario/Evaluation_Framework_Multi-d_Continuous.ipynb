{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Learning Solution Evaluation Framework\n",
    "\n",
    "To evaluate different solutions for the optimization problem in the multi-dimensional continuous space, we provide this evaluation framework. This framework is designed to use the simulator as the data source. \n",
    "\n",
    "The structure of this framework:\n",
    "* [Part 1. Data](#data): \n",
    "    * Load data from the simulator outputs\n",
    "* [Part 2. Solution](#solution): \n",
    "    * Run the solution on data loaded in Part1 and export the trajectory\n",
    "* [Part 3. Evaluation](#evaluation): \n",
    "    * Evaluate the trajectory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Data <a name=\"data\"></a> \n",
    "\n",
    "First, we will load all outputs from the simulator as potential inputs. Please modify `data_folder` to where the files are located. The three files are:\n",
    "* Simulated dataset (.csv)\n",
    "* Ground truth summary file (.csv)\n",
    "* Simulator configs (.json)\n",
    "\n",
    "If you did not use the default file names from the simulator, please provide the file names in the `Load data` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder\n",
    "data_folder = r'E:\\data\\20200214_vector_plearning\\data_example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data: dataset, ground truth summary, config file\n",
    "path_data = os.path.join(data_folder, 'simulation_data_all.csv')\n",
    "path_summary = os.path.join(data_folder, 'simulation_data_summary.csv')\n",
    "path_config = os.path.join(data_folder, 'simulation_data_configs.json')\n",
    "sim_data = pd.read_csv(path_data)\n",
    "sim_summary = pd.read_csv(path_summary)\n",
    "sim_config = json.load(open(path_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Solution <a name=\"solution\"></a> \n",
    "\n",
    "In this section, provide your solution. The solution can take any file(s) loaded above as input(s), and return the trajectory as output. The trajectory will be exported to file for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# PLACEHOLDER FOR YOUR SOLUTION\n",
    "# -------------------------------------------\n",
    "# Solution: generate the trajectory along the way\n",
    "from solution import *\n",
    "trajectory = Solution.gen_trajectory(sim_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output trajectory to file\n",
    "path_trajectory = os.path.join(data_folder, 'simulation_data_solution_trajectory.csv')\n",
    "trajectory.to_csv(path_trajectory, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Evaluation <a name=\"evaluation\"></a> \n",
    "\n",
    "In this section, we will evaluate the regret according to the trajectory. The report includes:\n",
    "* Total_N: Total number of samples explored.\n",
    "* Optimal_Reward: The best average reward from the ground truth file.\n",
    "* Last_5_Rewards_Avg: The average reward from the last 5 samples in a trajectory\n",
    "* Diff_from_Optimal: The difference between Last_5_Rewards_Avg and Optimal_Reward in terms of %.\n",
    "* Total_Regret: The sum of the regrets in a trajectory\n",
    "* Avg_Regret: The average regrets for each sample in a trajectory\n",
    "\n",
    "The evaluation function takes the trajectory file, the ground truth summary file and the optimization direction (min or max) as inputs. \n",
    "\n",
    "By default, we assume that the trajectory does not include rewards. So the evaluation function will find the closest configuration from the ground truth file, and use its reward as an approximation for each configuration in the trajectory. If you prefer to record the reward you get in the trajectory file, in the evaluation step, you can set `debug=True` so that no approximation will be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory evaluation\n",
    "from trajectory_evaluation import *\n",
    "te = TrajectoryEvaluation(path_trajectory, path_summary, 'min', debug=False)\n",
    "df_evaluation = te.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(df_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
